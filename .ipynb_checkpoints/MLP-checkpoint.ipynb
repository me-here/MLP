{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal 1: Create perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, W, b):\n",
    "    '''\n",
    "    Input: weights W, biases b, input activations x\n",
    "    Output: Single hypothesis\n",
    "    '''\n",
    "    z = np.sum(W.T * x) + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def deriv_sigmoid(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9886136869584284"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.seed(10)\n",
    "\n",
    "activations = np.array([0.3, 0.1, 0.7, 8])\n",
    "weights = np.random.rand(activations.shape[0])\n",
    "bias = 1\n",
    "\n",
    "perceptron(activations, weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal 2: Get, Clean, & Normalize Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LotArea  OverallQual  OverallCond  TotalBsmtSF  FullBath  HalfBath  \\\n",
      "0     8450            7            5          856         2         1   \n",
      "1     9600            6            8         1262         2         0   \n",
      "2    11250            7            5          920         2         1   \n",
      "3     9550            7            5          756         1         0   \n",
      "4    14260            8            5         1145         2         1   \n",
      "\n",
      "   BedroomAbvGr  TotRmsAbvGrd  Fireplaces  GarageArea  AboveMedianPrice  \n",
      "0             3             8           0         548                 1  \n",
      "1             3             6           1         460                 1  \n",
      "2             3             6           1         608                 1  \n",
      "3             3             7           1         642                 0  \n",
      "4             4             9           1         836                 1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"housepricedata.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8450     7     5 ...     8     0   548]\n",
      " [ 9600     6     8 ...     6     1   460]\n",
      " [11250     7     5 ...     6     1   608]\n",
      " ...\n",
      " [ 9042     7     9 ...     9     2   252]\n",
      " [ 9717     5     6 ...     5     0   240]\n",
      " [ 9937     5     6 ...     6     0   276]]\n"
     ]
    }
   ],
   "source": [
    "houses = data.values # DataFrame --> Array\n",
    "x = houses[:, :-1] # Input Activations\n",
    "y = houses[:, -1] # Output labels (0 or 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x = MinMaxScaler().fit(x).transform(x) # features between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valtest, Y_train, Y_valtest = train_test_split(x, y, test_size=0.3) # 70% Train\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_valtest, Y_valtest, test_size=0.5) # 15% to test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1022, 10)\n",
      "(219, 10)\n",
      "(219, 10)\n",
      "(1460, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal 3: Forward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(0)\n",
    "def forwardProp(activations, weights, biases, zArr, numLayers):\n",
    "    '''\n",
    "    Vectorized Implementation\n",
    "    Input: First Layer activations, weights, biases, number of layers\n",
    "    Output: Last layer\n",
    "    '''\n",
    "    # Layer i\n",
    "    for i in range(numLayers-1):\n",
    "        z = np.dot(weights[i], activations[i]) + biases[i]\n",
    "        #print(weights[i].shape, activations[i].shape, \"-->\", z.shape)\n",
    "        zArr.append(z)\n",
    "        activations.append(sigmoid(z))\n",
    "\n",
    "params = {\n",
    "    \"weights\": [\n",
    "    np.random.randn(X_train.shape[0], X_train.shape[1]) * np.sqrt(2/X_train.shape[0]), # l1\n",
    "    np.random.randn(1, X_train.shape[0]) * np.sqrt(2/X_train.shape[0])  # l2\n",
    "    ],\n",
    "    \n",
    "    \"biases\": np.ones(numLayers-1),\n",
    "    \n",
    "    \"numLayers\": 3,\n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "numLayers = 3\n",
    "zArr = []\n",
    "activations = [X_train.T]\n",
    "weights = [\n",
    "    np.random.randn(X_train.shape[0], X_train.shape[1]) * np.sqrt(2/X_train.shape[0]), # l1\n",
    "    np.random.randn(1, X_train.shape[0]) * np.sqrt(2/X_train.shape[0])  # l2\n",
    "          ]\n",
    "biases = np.ones(numLayers-1) # bias in each layer except output\n",
    "\n",
    "forwardProp(activations, weights, biases, zArr, numLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03729544  0.1334934   0.1332157  ... -0.03700293 -0.03773575\n",
      "   0.13321245]]\n"
     ]
    }
   ],
   "source": [
    "def backwardProp(activations, weights, biases, numLayers, actual, zArr, weightDecay, learning_rate):\n",
    "    # Compute Cost\n",
    "    cost = np.mean(1/2 * np.linalg.norm(activations[-1]-actual) ** 2)\n",
    "    for layer in range(numLayers - 1):\n",
    "        for i in range(activations[layer].shape[0]):\n",
    "            for j in range(activations[layer+1].shape[0]):\n",
    "                cost += weights[layer][j][i] ** 2\n",
    "    cost *= weightDecay / 2\n",
    "    \n",
    "    # Output layer delta\n",
    "    deltas = {}\n",
    "    delta_nL = np.multiply(-(actual - activations[-1]), deriv_sigmoid(zArr[-1]))\n",
    "    deltas[2] = delta_nL\n",
    "    \n",
    "    # Go through hidden layers (l2 --> index 1)\n",
    "    for l in range(numLayers-2, 0, -1): \n",
    "        deltas[l] = np.dot(weights[l].T, deltas[l+1]) * deriv_sigmoid(zArr[l])\n",
    "        deriv_W = np.dot(deltas[l+1], activations[l].T)\n",
    "        deriv_b = deltas[l+1]\n",
    "        #print(deriv_W, deriv_b)\n",
    "        \n",
    "        weights[l] = weights[l] - learning_rate * deriv_W\n",
    "        print(deriv_b[0])\n",
    "        #biases[l] = biases[l] - learning_rate * deriv_b\n",
    "    \n",
    "weightDecay = 0.5\n",
    "learning_rate = 0.05\n",
    "backwardProp(activations, weights, biases, numLayers, Y_train, zArr, weightDecay, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "1. http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "2. https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
